{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary libraries and configure the environment for the data cleaning and preprocessing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the NBA shots data from CSV files for the years 2019 to 2023 and combine them into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "dfs = {'combined': pd.DataFrame()}  # Initialize a combined DataFrame\n",
    "\n",
    "for year in years:\n",
    "    file_path = f\"data/NBA_{year}_Shots.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs[year] = df  # Store the DataFrame in the dictionary\n",
    "        dfs['combined'] = pd.concat([dfs['combined'], df], ignore_index=True)  # Update the combined DataFrame\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file 'NBA_{year}_Shots.csv' for {year} does not exist.\")\n",
    "\n",
    "\n",
    "# To select a specific year (e.g., 2020), you can access it like this:\n",
    "\n",
    "df2019 = dfs[2019]\n",
    "df2020 = dfs[2020]\n",
    "df2021 = dfs[2021]\n",
    "df2022 = dfs[2022]\n",
    "df2023 = dfs[2023]\n",
    "\n",
    "# To select the combined data, you can access it like this:\n",
    "dfcombined = dfs['combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    " df2015 = pd.read_csv(\"data/NBA_2015_Shots.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Identify and handle missing values in the dataset to ensure data quality for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = dfcombined.isnull().sum()\n",
    "missing_values_2015 = df2015.isnull().sum()\n",
    "# Print columns with missing values and their respective counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(POSITION_GROUP    0.513802\n",
       " POSITION          0.513802\n",
       " dtype: float64,\n",
       " 60)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling missing values: Check the percentage of missing values in 'POSITION_GROUP' and 'POSITION'\n",
    "missing_values_percentage = dfcombined[['POSITION_GROUP', 'POSITION']].isnull().mean() * 100\n",
    "\n",
    "# Handling duplicate rows: Check for any duplicate rows in the data\n",
    "duplicate_rows = dfcombined.duplicated().sum()\n",
    "\n",
    "# Output the percentage of missing values and the number of duplicate rows\n",
    "(missing_values_percentage, duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(POSITION_GROUP    0.055461\n",
       " POSITION          0.055461\n",
       " dtype: float64,\n",
       " 0)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling missing values: Check the percentage of missing values in 'POSITION_GROUP' and 'POSITION'\n",
    "missing_values_percentage_2015 = df2015[['POSITION_GROUP', 'POSITION']].isnull().mean() * 100\n",
    "\n",
    "# Handling duplicate rows: Check for any duplicate rows in the data\n",
    "duplicate_rows_2015 = df2015.duplicated().sum()\n",
    "\n",
    "# Output the percentage of missing values and the number of duplicate rows\n",
    "(missing_values_percentage_2015, duplicate_rows_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using Z-score\n",
    "\n",
    "# Outlier detection for 'LOC_X'\n",
    "z_scores_loc_x = np.abs(stats.zscore(dfcombined['LOC_X']))\n",
    "outliers_loc_x = np.where(z_scores_loc_x > 3)\n",
    "\n",
    "\n",
    "# Outlier detection for 'LOC_Y'\n",
    "z_scores_loc_y = np.abs(stats.zscore(dfcombined['LOC_Y']))\n",
    "outliers_loc_y = np.where(z_scores_loc_y > 3)\n",
    "\n",
    "\n",
    "# Outlier detection for 'SHOT_DISTANCE'\n",
    "z_scores_shot_distance = np.abs(stats.zscore(dfcombined['SHOT_DISTANCE']))\n",
    "outliers_shot_distance = np.where(z_scores_shot_distance > 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using Z-score\n",
    "\n",
    "# Outlier detection for 'LOC_X'\n",
    "z_scores_loc_x_2015 = np.abs(stats.zscore(df2015['LOC_X']))\n",
    "outliers_loc_x_2015 = np.where(z_scores_loc_x_2015 > 3)\n",
    "\n",
    "\n",
    "# Outlier detection for 'LOC_Y'\n",
    "z_scores_loc_y_2015 = np.abs(stats.zscore(df2015['LOC_Y']))\n",
    "outliers_loc_y_2015 = np.where(z_scores_loc_y_2015 > 3)\n",
    "\n",
    "\n",
    "# Outlier detection for 'SHOT_DISTANCE'\n",
    "z_scores_shot_distance_2015 = np.abs(stats.zscore(df2015['SHOT_DISTANCE']))\n",
    "outliers_shot_distance_2015 = np.where(z_scores_shot_distance_2015 > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique player names associated with each player ID\n",
    "player_id_name = dfcombined.groupby('PLAYER_ID')['PLAYER_NAME'].unique()\n",
    "\n",
    "# Identify player IDs with more than one associated name\n",
    "inconsistent_players = player_id_name[player_id_name.apply(len) > 1]\n",
    "\n",
    "# Identify unique team names associated with each team ID\n",
    "team_id_name = dfcombined.groupby('TEAM_ID')['TEAM_NAME'].unique()\n",
    "\n",
    "# Identify team IDs with more than one associated name\n",
    "inconsistent_teams = team_id_name[team_id_name.apply(len) > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique player names associated with each player ID\n",
    "player_id_name_2015 = df2015.groupby('PLAYER_ID')['PLAYER_NAME'].unique()\n",
    "\n",
    "# Identify player IDs with more than one associated name\n",
    "inconsistent_players_2015 = player_id_name_2015[player_id_name_2015.apply(len) > 1]\n",
    "\n",
    "# Identify unique team names associated with each team ID\n",
    "team_id_name_2015 = df2015.groupby('TEAM_ID')['TEAM_NAME'].unique()\n",
    "\n",
    "# Identify team IDs with more than one associated name\n",
    "inconsistent_teams_2015 = team_id_name_2015[team_id_name_2015.apply(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate rows\n",
    "dfcombined_cleaned = dfcombined.drop_duplicates()\n",
    "\n",
    "# Since the percentage of missing values is small, we will drop the rows with missing 'POSITION_GROUP' and 'POSITION'\n",
    "dfcombined_cleaned = dfcombined_cleaned.dropna(subset=['POSITION_GROUP', 'POSITION'])\n",
    "\n",
    "# Corrections for player names\n",
    "player_name_corrections = {\n",
    "    1630197: \"Aleksej Pokusevski\",\n",
    "    1630527: \"Brandon Boston Jr.\",\n",
    "    1628408: \"PJ Dozier\",\n",
    "    1628384: \"OG Anunoby\",\n",
    "    1630214: \"Xavier Tillman Sr.\",\n",
    "    1630288: \"Jeff Dowtin Jr.\"\n",
    "}\n",
    "\n",
    "# Apply the corrections for player names\n",
    "dfcombined_cleaned['PLAYER_NAME'] = dfcombined_cleaned.apply(lambda row: player_name_corrections[row['PLAYER_ID']] if row['PLAYER_ID'] in player_name_corrections else row['PLAYER_NAME'], axis=1)\n",
    "\n",
    "# Corrections for team names\n",
    "team_name_corrections = {\n",
    "    \"LA Clippers\": \"Los Angeles Clippers\"\n",
    "}\n",
    "\n",
    "# Apply the corrections for team names\n",
    "dfcombined_cleaned['TEAM_NAME'] = dfcombined_cleaned['TEAM_NAME'].replace(team_name_corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate rows\n",
    "dfcombined_cleaned_2015 = df2015.drop_duplicates()\n",
    "\n",
    "# Since the percentage of missing values is small, we will drop the rows with missing 'POSITION_GROUP' and 'POSITION'\n",
    "dfcombined_cleaned_2015 = dfcombined_cleaned_2015.dropna(subset=['POSITION_GROUP', 'POSITION'])\n",
    "\n",
    "# Corrections for player names\n",
    "player_name_corrections_2015 = {\n",
    "    1630197: \"Aleksej Pokusevski\",\n",
    "    1630527: \"Brandon Boston Jr.\",\n",
    "    1628408: \"PJ Dozier\",\n",
    "    1628384: \"OG Anunoby\",\n",
    "    1630214: \"Xavier Tillman Sr.\",\n",
    "    1630288: \"Jeff Dowtin Jr.\"\n",
    "}\n",
    "\n",
    "# Apply the corrections for player names\n",
    "dfcombined_cleaned_2015['PLAYER_NAME'] = dfcombined_cleaned_2015.apply(lambda row: player_name_corrections_2015[row['PLAYER_ID']] if row['PLAYER_ID'] in player_name_corrections_2015 else row['PLAYER_NAME'], axis=1)\n",
    "\n",
    "# Corrections for team names\n",
    "team_name_corrections_2015 = {\n",
    "    \"LA Clippers\": \"Los Angeles Clippers\"\n",
    "}\n",
    "\n",
    "# Apply the corrections for team names\n",
    "dfcombined_cleaned_2015['TEAM_NAME'] = dfcombined_cleaned_2015['TEAM_NAME'].replace(team_name_corrections_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "##### Feature Description\n",
    "\n",
    "Before transforming our features to be suitable for machine learning algorithms, let's summarize the characteristics of our numeric and categorical variables post-cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Players & Teams with their IDs and zone names with its abbreviaton \n",
    "\n",
    "# Extract unique team IDs\n",
    "unique_team_ids = dfcombined_cleaned['TEAM_ID'].unique()\n",
    "\n",
    "# Extract unique team names\n",
    "unique_team_names = dfcombined_cleaned['TEAM_NAME'].unique()\n",
    "\n",
    "# Extract unique team abbreviations from the 'HOME_TEAM' column\n",
    "unique_home_teams = dfcombined_cleaned['HOME_TEAM'].unique()\n",
    "\n",
    "# Extract unique player IDs\n",
    "unique_player_ids = dfcombined_cleaned['PLAYER_ID'].unique()\n",
    "\n",
    "# Extract unique player names\n",
    "unique_player_names = dfcombined_cleaned['PLAYER_NAME'].unique()\n",
    "\n",
    "# Extract unique zone names\n",
    "unique_zone_names = dfcombined_cleaned['ZONE_NAME'].unique()\n",
    "\n",
    "# Extract unique zone abbreviations\n",
    "unique_zone_abb = dfcombined_cleaned['ZONE_ABB'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Players & Teams with their IDs and zone names with its abbreviaton \n",
    "\n",
    "# Extract unique team IDs\n",
    "unique_team_ids_2015 = dfcombined_cleaned_2015['TEAM_ID'].unique()\n",
    "\n",
    "# Extract unique team names\n",
    "unique_team_names_2015 = dfcombined_cleaned_2015['TEAM_NAME'].unique()\n",
    "\n",
    "# Extract unique team abbreviations from the 'HOME_TEAM' column\n",
    "unique_home_teams_2015 = dfcombined_cleaned_2015['HOME_TEAM'].unique()\n",
    "\n",
    "# Extract unique player IDs\n",
    "unique_player_ids_2015 = dfcombined_cleaned_2015['PLAYER_ID'].unique()\n",
    "\n",
    "# Extract unique player names\n",
    "unique_player_names_2015 = dfcombined_cleaned_2015['PLAYER_NAME'].unique()\n",
    "\n",
    "# Extract unique zone names\n",
    "unique_zone_names_2015 = dfcombined_cleaned_2015['ZONE_NAME'].unique()\n",
    "\n",
    "# Extract unique zone abbreviations\n",
    "unique_zone_abb_2015 = dfcombined_cleaned_2015['ZONE_ABB'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Is_Home_Team binary column in order to not double count the home/away feature and making it a numerical to save \n",
    "# memory + easier for future usage\n",
    "\n",
    "# Mapping dictionaries to convert team, player identifiers and zone names to names and abbreviations.\n",
    "team_id_to_name_mapping = dict(zip(unique_team_ids, unique_team_names))\n",
    "team_abbreviation_to_id_mapping = dict(zip(unique_home_teams, unique_team_ids))\n",
    "player_id_to_name_mapping = dict(zip(unique_player_ids, unique_player_names))\n",
    "zone_abb_to_zonename_mapping = dict(zip(unique_zone_abb, unique_zone_names))\n",
    "\n",
    "# Mapping team abbreviations to numeric team IDs for the 'HOME_TEAM' column.\n",
    "team_abbr_to_id = {\n",
    "    'LAC': 1610612762, 'ATL': 1610612746, 'MIL': 1610612737,\n",
    "    'BKN': 1610612754, 'SAS': 1610612760, 'MEM': 1610612749,\n",
    "    'NYK': 1610612751, 'PHI': 1610612748, 'DEN': 1610612742,\n",
    "    'CHA': 1610612759, 'POR': 1610612744, 'DAL': 1610612752,\n",
    "    'UTA': 1610612763, 'MIA': 1610612765, 'OKC': 1610612741,\n",
    "    'LAL': 1610612750, 'DET': 1610612755, 'WAS': 1610612743,\n",
    "    'MIN': 1610612766, 'NOP': 1610612753, 'CHI': 1610612758,\n",
    "    'CLE': 1610612757, 'GSW': 1610612756, 'TOR': 1610612745,\n",
    "    'SAC': 1610612747, 'BOS': 1610612764, 'HOU': 1610612738,\n",
    "    'IND': 1610612761, 'ORL': 1610612740, 'PHX': 1610612739\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'HOME_TEAM' column to create a 'Home_Team_ID' column.\n",
    "dfcombined_cleaned['Home_Team_ID'] = dfcombined_cleaned['HOME_TEAM'].map(team_abbr_to_id)\n",
    "\n",
    "# Determine if the team is playing at home and encode this in a new 'Is_Home_Team' column.\n",
    "dfcombined_cleaned['IS_HOME_TEAM'] = (dfcombined_cleaned['Home_Team_ID'] == dfcombined_cleaned['TEAM_ID']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Is_Home_Team binary column in order to not double count the home/away feature and making it a numerical to save \n",
    "# memory + easier for future usage\n",
    "\n",
    "# Mapping dictionaries to convert team, player identifiers and zone names to names and abbreviations.\n",
    "team_id_to_name_mapping_2015 = dict(zip(unique_team_ids_2015, unique_team_names_2015))\n",
    "team_abbreviation_to_id_mapping_2015 = dict(zip(unique_home_teams_2015, unique_team_ids_2015))\n",
    "player_id_to_name_mapping_2015 = dict(zip(unique_player_ids_2015, unique_player_names_2015))\n",
    "zone_abb_to_zonename_mapping_2015 = dict(zip(unique_zone_abb_2015, unique_zone_names_2015))\n",
    "\n",
    "# Mapping team abbreviations to numeric team IDs for the 'HOME_TEAM' column.\n",
    "team_abbr_to_id_2015 = {\n",
    "    'LAC': 1610612762, 'ATL': 1610612746, 'MIL': 1610612737,\n",
    "    'BKN': 1610612754, 'SAS': 1610612760, 'MEM': 1610612749,\n",
    "    'NYK': 1610612751, 'PHI': 1610612748, 'DEN': 1610612742,\n",
    "    'CHA': 1610612759, 'POR': 1610612744, 'DAL': 1610612752,\n",
    "    'UTA': 1610612763, 'MIA': 1610612765, 'OKC': 1610612741,\n",
    "    'LAL': 1610612750, 'DET': 1610612755, 'WAS': 1610612743,\n",
    "    'MIN': 1610612766, 'NOP': 1610612753, 'CHI': 1610612758,\n",
    "    'CLE': 1610612757, 'GSW': 1610612756, 'TOR': 1610612745,\n",
    "    'SAC': 1610612747, 'BOS': 1610612764, 'HOU': 1610612738,\n",
    "    'IND': 1610612761, 'ORL': 1610612740, 'PHX': 1610612739\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'HOME_TEAM' column to create a 'Home_Team_ID' column.\n",
    "dfcombined_cleaned_2015['Home_Team_ID'] = dfcombined_cleaned_2015['HOME_TEAM'].map(team_abbr_to_id_2015)\n",
    "\n",
    "# Determine if the team is playing at home and encode this in a new 'Is_Home_Team' column.\n",
    "dfcombined_cleaned_2015['IS_HOME_TEAM'] = (dfcombined_cleaned_2015['Home_Team_ID'] == dfcombined_cleaned_2015['TEAM_ID']).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date Feature Engineering\n",
    "The GAME_DATE column is converted to a datetime object to extract additional features that may be relevant for analysis, such as the day of the week and month of the game. The year is already given in SEASON_1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'GAME_DATE' column to a datetime object and extracting relevant features (year not needed = SEASON_1)\n",
    "dfcombined_cleaned['GAME_DATE'] = pd.to_datetime(dfcombined_cleaned['GAME_DATE'])\n",
    "\n",
    "# Extracting the month as a number (1-12)\n",
    "dfcombined_cleaned['MONTH'] = dfcombined_cleaned['GAME_DATE'].dt.month\n",
    "\n",
    "# Extracting the day of the month as a number (1-31)\n",
    "dfcombined_cleaned['DAY'] = dfcombined_cleaned['GAME_DATE'].dt.day\n",
    "\n",
    "# Extracting the day of the week as a number (1-7, where Monday=1, Sunday=7)\n",
    "dfcombined_cleaned['WEEKDAY'] = dfcombined_cleaned['GAME_DATE'].dt.dayofweek + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'GAME_DATE' column to a datetime object and extracting relevant features (year not needed = SEASON_1)\n",
    "dfcombined_cleaned_2015['GAME_DATE'] = pd.to_datetime(dfcombined_cleaned_2015['GAME_DATE'])\n",
    "\n",
    "# Extracting the month as a number (1-12)\n",
    "dfcombined_cleaned_2015['MONTH'] = dfcombined_cleaned_2015['GAME_DATE'].dt.month\n",
    "\n",
    "# Extracting the day of the month as a number (1-31)\n",
    "dfcombined_cleaned_2015['DAY'] = dfcombined_cleaned_2015['GAME_DATE'].dt.day\n",
    "\n",
    "# Extracting the day of the week as a number (1-7, where Monday=1, Sunday=7)\n",
    "dfcombined_cleaned_2015['WEEKDAY'] = dfcombined_cleaned_2015['GAME_DATE'].dt.dayofweek + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of TIME_LEFT Column\n",
    "\n",
    "The `TIME_LEFT` column is computed by converting the minutes left (`MINS_LEFT`) to seconds and adding the remaining seconds (`SECS_LEFT`). This transformation facilitates the analysis of time-related patterns in the data by representing the time left in the game as a single numeric value in seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>MINS_LEFT</th>\n",
       "      <th>SECS_LEFT</th>\n",
       "      <th>TOTAL_TIME_LEFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    QUARTER  MINS_LEFT  SECS_LEFT  TOTAL_TIME_LEFT\n",
       "0         5          0         18               18\n",
       "1         5          0         27               27\n",
       "2         4          0          2                2\n",
       "3         5          0         46               46\n",
       "4         5          0         55               55\n",
       "5         5          1          3               63\n",
       "6         5          1         16               76\n",
       "7         4          0          7                7\n",
       "8         5          1         32               92\n",
       "9         5          1         39               99\n",
       "10        5          1         45              105\n",
       "11        5          1         59              119\n",
       "12        4          0         29               29\n",
       "13        5          2         11              131\n",
       "14        4          0         36               36"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'MINS_LEFT' to seconds and add 'SECS_LEFT' to create 'TIME_LEFT' in seconds\n",
    "#dfcombined_cleaned['TIME_LEFT'] = dfcombined_cleaned['MINS_LEFT'] * 60 + dfcombined_cleaned['SECS_LEFT']\n",
    "# TOTAL_TIME_LEFT including overtime\n",
    "\n",
    "dfcombined_cleaned['TOTAL_TIME_LEFT'] = dfcombined_cleaned.apply(\n",
    "    lambda row: (((4 - row['QUARTER']) * 12 * 60) + (row['MINS_LEFT'] * 60) + row['SECS_LEFT']) \n",
    "    if row['QUARTER'] <= 4 \n",
    "    else ((row['MINS_LEFT'] * 60) + row['SECS_LEFT']), axis=1)\n",
    "\n",
    "# Re-display the first few rows to check the changes\n",
    "dfcombined_cleaned[['QUARTER', 'MINS_LEFT', 'SECS_LEFT', 'TOTAL_TIME_LEFT']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>MINS_LEFT</th>\n",
       "      <th>SECS_LEFT</th>\n",
       "      <th>TOTAL_TIME_LEFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    QUARTER  MINS_LEFT  SECS_LEFT  TOTAL_TIME_LEFT\n",
       "0         4          0         18               18\n",
       "1         4          0         22               22\n",
       "2         5          0          3                3\n",
       "3         4          0         37               37\n",
       "4         4          0         48               48\n",
       "5         4          1          2               62\n",
       "6         4          1         12               72\n",
       "7         4          0          0                0\n",
       "8         4          0         23               23\n",
       "9         4          0         46               46\n",
       "10        4          0         30               30\n",
       "11        5          0         10               10\n",
       "12        5          0         29               29\n",
       "13        4          0         41               41\n",
       "14        4          0         55               55"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'MINS_LEFT' to seconds and add 'SECS_LEFT' to create 'TIME_LEFT' in seconds\n",
    "#dfcombined_cleaned_2015['TIME_LEFT'] = dfcombined_cleaned_2015['MINS_LEFT'] * 60 + dfcombined_cleaned_2015['SECS_LEFT']\n",
    "# TOTAL_TIME_LEFT including overtime\n",
    "\n",
    "dfcombined_cleaned_2015['TOTAL_TIME_LEFT'] = dfcombined_cleaned_2015.apply(\n",
    "    lambda row: (((4 - row['QUARTER']) * 12 * 60) + (row['MINS_LEFT'] * 60) + row['SECS_LEFT']) \n",
    "    if row['QUARTER'] <= 4 \n",
    "    else ((row['MINS_LEFT'] * 60) + row['SECS_LEFT']), axis=1)\n",
    "\n",
    "# Re-display the first few rows to check the changes\n",
    "dfcombined_cleaned_2015[['QUARTER', 'MINS_LEFT', 'SECS_LEFT', 'TOTAL_TIME_LEFT']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization of LOC_X and LOC_Y\n",
    "LOC_X and LOC_Y show different scalings throughout the year. This makes it hard to compare the different years, which is why in the following the values are being normalized to scale LOC_X and LOC_Y to the scale of a basketball court."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NORM_LOC_X</th>\n",
       "      <th>NORM_LOC_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NORM_LOC_X  NORM_LOC_Y\n",
       "min       -25.0         0.0\n",
       "max        25.0        94.0"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization function to scale LOC_X and LOC_Y values\n",
    "def normalize_coordinates(dataframe, new_x_range=(-25, 25), new_y_range=(0, 94)):\n",
    "    \"\"\"\n",
    "    This function normalizes LOC_X and LOC_Y values to a given range.\n",
    "    The new_x_range and new_y_range parameters define the desired output range for LOC_X and LOC_Y.\n",
    "    \"\"\"\n",
    "    x_min, x_max = new_x_range\n",
    "    y_min, y_max = new_y_range\n",
    "    \n",
    "    # Normalize LOC_X\n",
    "    x_range = dataframe['LOC_X'].max() - dataframe['LOC_X'].min()\n",
    "    dataframe['NORM_LOC_X'] = ((dataframe['LOC_X'] - dataframe['LOC_X'].min()) / x_range) * (x_max - x_min) + x_min\n",
    "    \n",
    "    # Normalize LOC_Y\n",
    "    y_range = dataframe['LOC_Y'].max() - dataframe['LOC_Y'].min()\n",
    "    dataframe['NORM_LOC_Y'] = ((dataframe['LOC_Y'] - dataframe['LOC_Y'].min()) / y_range) * (y_max - y_min) + y_min\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Now we will group by 'SEASON_1' and normalize within each season\n",
    "df_list = []\n",
    "for season, group in dfcombined_cleaned.groupby('SEASON_1'):\n",
    "    normalized_group = normalize_coordinates(group)\n",
    "    df_list.append(normalized_group)\n",
    "\n",
    "# Concatenate all the normalized groups back into a single dataframe\n",
    "dfcombined_cleaned = pd.concat(df_list)\n",
    "\n",
    "# Verify normalization by checking the range of values for the first season\n",
    "dfcombined_cleaned[dfcombined_cleaned['SEASON_1'] == 2019][['NORM_LOC_X', 'NORM_LOC_Y']].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NORM_LOC_X</th>\n",
       "      <th>NORM_LOC_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NORM_LOC_X  NORM_LOC_Y\n",
       "min       -25.0         0.0\n",
       "max        25.0        94.0"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization function to scale LOC_X and LOC_Y values\n",
    "def normalize_coordinates(dataframe, new_x_range=(-25, 25), new_y_range=(0, 94)):\n",
    "    \"\"\"\n",
    "    This function normalizes LOC_X and LOC_Y values to a given range.\n",
    "    The new_x_range and new_y_range parameters define the desired output range for LOC_X and LOC_Y.\n",
    "    \"\"\"\n",
    "    x_min, x_max = new_x_range\n",
    "    y_min, y_max = new_y_range\n",
    "    \n",
    "    # Normalize LOC_X\n",
    "    x_range = dataframe['LOC_X'].max() - dataframe['LOC_X'].min()\n",
    "    dataframe['NORM_LOC_X'] = ((dataframe['LOC_X'] - dataframe['LOC_X'].min()) / x_range) * (x_max - x_min) + x_min\n",
    "    \n",
    "    # Normalize LOC_Y\n",
    "    y_range = dataframe['LOC_Y'].max() - dataframe['LOC_Y'].min()\n",
    "    dataframe['NORM_LOC_Y'] = ((dataframe['LOC_Y'] - dataframe['LOC_Y'].min()) / y_range) * (y_max - y_min) + y_min\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Now we will group by 'SEASON_1' and normalize within each season\n",
    "df_list_2015 = []\n",
    "for season, group in dfcombined_cleaned_2015.groupby('SEASON_1'):\n",
    "    normalized_group = normalize_coordinates(group)\n",
    "    df_list_2015.append(normalized_group)\n",
    "\n",
    "# Concatenate all the normalized groups back into a single dataframe\n",
    "dfcombined_cleaned_2015 = pd.concat(df_list_2015)\n",
    "\n",
    "# Verify normalization by checking the range of values for the first season\n",
    "dfcombined_cleaned_2015[dfcombined_cleaned_2015['SEASON_1'] == 2015][['NORM_LOC_X', 'NORM_LOC_Y']].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction and Type Conversion\n",
    "\n",
    "##### Reducing Redundant Features\n",
    "To streamline the dataset, redundant or unnecessary features are removed. This includes dropping columns that are represented by other, more effective identifiers or that have been encoded into new features.\n",
    "\n",
    "##### Columns Dropped\n",
    "- `PLAYER_NAME` and `TEAM_NAME` are removed in favor of `PLAYER_ID` and `TEAM_ID`.\n",
    "- `SEASON_2` is redundant due to `SEASON_1`, which will also be renamed to `SEASON` .\n",
    "- `ZONE_NAME` is represented by `ZONE_ABB`.\n",
    "- `EVENT_TYPE` is captured by the binary `SHOT_MADE`.\n",
    "- `Game_ID` is excluded because it is specific to past games and will not be applicable for future shot predictions. It merely indicates that shots were taken in the same game, which is not useful for our predictive modeling.\n",
    "- `Home_Team_ID`, `HOME_TEAM`, and `AWAY_TEAM` are removed as they have been replaced by the binary `Is_Home_Team` column, which simplifies the dataset and avoids duplicative information.\n",
    "- `MINS_LEFT` and `SECS_LEFT` are consolidated into `TIME_LEFT` in seconds, which makes the original columns unnecessary.\n",
    "- `GAME_DATE` is already captured in `SEASON`, `Month` and `Day` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns\n",
    "\n",
    "dfcombined_cleaned = dfcombined_cleaned.drop(['PLAYER_NAME', 'TEAM_NAME', 'SEASON_2', 'ZONE_NAME', 'EVENT_TYPE', 'GAME_ID', 'Home_Team_ID','HOME_TEAM','AWAY_TEAM', 'MINS_LEFT', 'SECS_LEFT', 'GAME_DATE'], axis=1)\n",
    "\n",
    "# Rename 'SEASON_1' column to 'SEASON'\n",
    "dfcombined_cleaned.rename(columns={'SEASON_1': 'SEASON'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unecessary columns\n",
    "\n",
    "dfcombined_cleaned_2015 = dfcombined_cleaned_2015.drop(['PLAYER_NAME', 'TEAM_NAME', 'SEASON_2', 'ZONE_NAME', 'EVENT_TYPE', 'GAME_ID', 'Home_Team_ID','HOME_TEAM','AWAY_TEAM', 'MINS_LEFT', 'SECS_LEFT', 'GAME_DATE'], axis=1)\n",
    "\n",
    "# Rename 'SEASON_1' column to 'SEASON'\n",
    "dfcombined_cleaned_2015.rename(columns={'SEASON_1': 'SEASON'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Conversion\n",
    "Categorical columns are explicitly converted to categorical types to optimize memory usage and improve the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert SHOT_MADE to binary\n",
    "dfcombined_cleaned['SHOT_MADE'] = dfcombined_cleaned['SHOT_MADE'].astype(int)\n",
    "\n",
    "# Map SHOT_TYPE to binary\n",
    "shot_type_mapping = {'2PT Field Goal': 0, '3PT Field Goal': 1}\n",
    "dfcombined_cleaned['SHOT_TYPE'] = dfcombined_cleaned['SHOT_TYPE'].map(shot_type_mapping)\n",
    "\n",
    "# Convert SEASON and QUARTER to ordinal\n",
    "season_order = sorted(dfcombined_cleaned['SEASON'].unique())\n",
    "quarter_order = sorted(dfcombined_cleaned['QUARTER'].unique())\n",
    "dfcombined_cleaned['SEASON'] = pd.Categorical(dfcombined_cleaned['SEASON'], categories=season_order, ordered=True)\n",
    "dfcombined_cleaned['QUARTER'] = pd.Categorical(dfcombined_cleaned['QUARTER'], categories=quarter_order, ordered=True)\n",
    "\n",
    "# Convert MONTH and WEEKDAY to categorical\n",
    "dfcombined_cleaned['MONTH'] = dfcombined_cleaned['MONTH'].astype('category')\n",
    "dfcombined_cleaned['WEEKDAY'] = dfcombined_cleaned['WEEKDAY'].astype('category')\n",
    "\n",
    "# Leave DAY as numerical unless there's a need to treat it as categorical\n",
    "\n",
    "# Convert ACTION_TYPE, BASIC_ZONE, ZONE_ABB, and ZONE_RANGE to categorical\n",
    "categorical_columns = ['PLAYER_ID', 'TEAM_ID', 'ACTION_TYPE', 'BASIC_ZONE', 'ZONE_ABB', 'ZONE_RANGE', 'POSITION_GROUP', 'POSITION']\n",
    "for col in categorical_columns:\n",
    "    dfcombined_cleaned[col] = dfcombined_cleaned[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SHOT_MADE to binary\n",
    "dfcombined_cleaned_2015['SHOT_MADE'] = dfcombined_cleaned_2015['SHOT_MADE'].astype(int)\n",
    "\n",
    "# Map SHOT_TYPE to binary\n",
    "shot_type_mapping_2015 = {'2PT Field Goal': 0, '3PT Field Goal': 1}\n",
    "dfcombined_cleaned_2015['SHOT_TYPE'] = dfcombined_cleaned_2015['SHOT_TYPE'].map(shot_type_mapping_2015)\n",
    "\n",
    "# Convert SEASON and QUARTER to ordinal\n",
    "season_order_2015 = sorted(dfcombined_cleaned_2015['SEASON'].unique())\n",
    "quarter_order_2015 = sorted(dfcombined_cleaned_2015['QUARTER'].unique())\n",
    "dfcombined_cleaned_2015['SEASON'] = pd.Categorical(dfcombined_cleaned_2015['SEASON'], categories=season_order_2015, ordered=True)\n",
    "dfcombined_cleaned_2015['QUARTER'] = pd.Categorical(dfcombined_cleaned_2015['QUARTER'], categories=quarter_order_2015, ordered=True)\n",
    "\n",
    "# Convert MONTH and WEEKDAY to categorical\n",
    "dfcombined_cleaned_2015['MONTH'] = dfcombined_cleaned_2015['MONTH'].astype('category')\n",
    "dfcombined_cleaned_2015['WEEKDAY'] = dfcombined_cleaned_2015['WEEKDAY'].astype('category')\n",
    "\n",
    "# Leave DAY as numerical unless there's a need to treat it as categorical\n",
    "\n",
    "# Convert ACTION_TYPE, BASIC_ZONE, ZONE_ABB, and ZONE_RANGE to categorical\n",
    "categorical_columns_2015 = ['PLAYER_ID', 'TEAM_ID', 'ACTION_TYPE', 'BASIC_ZONE', 'ZONE_ABB', 'ZONE_RANGE', 'POSITION_GROUP', 'POSITION']\n",
    "for col in categorical_columns:\n",
    "    dfcombined_cleaned_2015[col] = dfcombined_cleaned_2015[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Optimization for Numerical Variables\n",
    "\n",
    "##### Reducing Data Type Sizes\n",
    "To enhance computational efficiency and reduce memory usage, numerical columns are cast to more memory-efficient data types. Float64 columns are converted to float32, and int64 columns are converted to int32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary columns to int32\n",
    "dfcombined_cleaned['SHOT_MADE'] = dfcombined_cleaned['SHOT_MADE'].astype('int32')\n",
    "dfcombined_cleaned['IS_HOME_TEAM'] = dfcombined_cleaned['IS_HOME_TEAM'].astype('int32')\n",
    "\n",
    "# Convert other integer columns to int32\n",
    "dfcombined_cleaned['DAY'] = dfcombined_cleaned['DAY'].astype('int32')\n",
    "#dfcombined_cleaned['TIME_LEFT'] = dfcombined_cleaned['TIME_LEFT'].astype('int32')\n",
    "dfcombined_cleaned['TOTAL_TIME_LEFT'] = dfcombined_cleaned['TOTAL_TIME_LEFT'].astype('int32')\n",
    "dfcombined_cleaned['SHOT_DISTANCE'] = dfcombined_cleaned['SHOT_DISTANCE'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary columns to int32\n",
    "dfcombined_cleaned_2015['SHOT_MADE'] = dfcombined_cleaned_2015['SHOT_MADE'].astype('int32')\n",
    "dfcombined_cleaned_2015['IS_HOME_TEAM'] = dfcombined_cleaned_2015['IS_HOME_TEAM'].astype('int32')\n",
    "\n",
    "# Convert other integer columns to int32\n",
    "dfcombined_cleaned_2015['DAY'] = dfcombined_cleaned_2015['DAY'].astype('int32')\n",
    "#dfcombined_cleaned_2015['TIME_LEFT'] = dfcombined_cleaned_2015['TIME_LEFT'].astype('int32')\n",
    "dfcombined_cleaned_2015['TOTAL_TIME_LEFT'] = dfcombined_cleaned_2015['TOTAL_TIME_LEFT'].astype('int32')\n",
    "dfcombined_cleaned_2015['SHOT_DISTANCE'] = dfcombined_cleaned_2015['SHOT_DISTANCE'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Integrity Check\n",
    "\n",
    "##### Verifying Data Types and Cardinality\n",
    "After preprocessing, it is crucial to verify that data types are correctly assigned and to understand the cardinality of categorical variables. This step ensures that the dataset is ready for encoding and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1027134 entries, 0 to 1032498\n",
      "Data columns (total 22 columns):\n",
      " #   Column           Non-Null Count    Dtype   \n",
      "---  ------           --------------    -----   \n",
      " 0   SEASON           1027134 non-null  category\n",
      " 1   TEAM_ID          1027134 non-null  category\n",
      " 2   PLAYER_ID        1027134 non-null  category\n",
      " 3   POSITION_GROUP   1027134 non-null  category\n",
      " 4   POSITION         1027134 non-null  category\n",
      " 5   SHOT_MADE        1027134 non-null  int32   \n",
      " 6   ACTION_TYPE      1027134 non-null  category\n",
      " 7   SHOT_TYPE        1027134 non-null  int64   \n",
      " 8   BASIC_ZONE       1027134 non-null  category\n",
      " 9   ZONE_ABB         1027134 non-null  category\n",
      " 10  ZONE_RANGE       1027134 non-null  category\n",
      " 11  LOC_X            1027134 non-null  float64 \n",
      " 12  LOC_Y            1027134 non-null  float64 \n",
      " 13  SHOT_DISTANCE    1027134 non-null  int32   \n",
      " 14  QUARTER          1027134 non-null  category\n",
      " 15  IS_HOME_TEAM     1027134 non-null  int32   \n",
      " 16  MONTH            1027134 non-null  category\n",
      " 17  DAY              1027134 non-null  int32   \n",
      " 18  WEEKDAY          1027134 non-null  category\n",
      " 19  TOTAL_TIME_LEFT  1027134 non-null  int32   \n",
      " 20  NORM_LOC_X       1027134 non-null  float64 \n",
      " 21  NORM_LOC_Y       1027134 non-null  float64 \n",
      "dtypes: category(12), float64(4), int32(5), int64(1)\n",
      "memory usage: 79.4 MB\n"
     ]
    }
   ],
   "source": [
    "#checking for Types and amount of unique values\n",
    "dfcombined_cleaned.info()\n",
    "\n",
    "# Define the categorical columns to be encoded\n",
    "categorical_columns = [\n",
    "    'TEAM_ID', 'PLAYER_ID', 'POSITION_GROUP', 'POSITION',\n",
    "    'ACTION_TYPE', 'BASIC_ZONE', 'ZONE_ABB', 'ZONE_RANGE', 'MONTH', 'WEEKDAY'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "#### Ordinal Encoding\n",
    "To utilize categorical features in machine learning models, we need to encode these features numerically. This process involves converting categories to a numerical format that preserves any inherent order in the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure SEASON is an ordered categorical type\n",
    "dfcombined_cleaned['SEASON'] = dfcombined_cleaned['SEASON'].cat.as_ordered()\n",
    "\n",
    "# Define the order for QUARTER, assuming 5, 6, 7, 8 are overtime periods\n",
    "quarter_mapping = {1: 'Q1', 2: 'Q2', 3: 'Q3', 4: 'Q4', 5: 'OT1', 6: 'OT2', 7: 'OT3', 8: 'OT4'}\n",
    "\n",
    "# Map the QUARTER column to the ordered categorical type\n",
    "dfcombined_cleaned['QUARTER'] = dfcombined_cleaned['QUARTER'].map(quarter_mapping).astype('category')\n",
    "\n",
    "# Set the categories and order for the QUARTER column\n",
    "dfcombined_cleaned['QUARTER'] = dfcombined_cleaned['QUARTER'].cat.set_categories(\n",
    "    ['Q1', 'Q2', 'Q3', 'Q4', 'OT1', 'OT2', 'OT3', 'OT4'], ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure SEASON is an ordered categorical type\n",
    "dfcombined_cleaned_2015['SEASON'] = dfcombined_cleaned_2015['SEASON'].cat.as_ordered()\n",
    "\n",
    "# Define the order for QUARTER, assuming 5, 6, 7, 8 are overtime periods\n",
    "quarter_mapping = {1: 'Q1', 2: 'Q2', 3: 'Q3', 4: 'Q4', 5: 'OT1', 6: 'OT2', 7: 'OT3', 8: 'OT4'}\n",
    "\n",
    "# Map the QUARTER column to the ordered categorical type\n",
    "dfcombined_cleaned_2015['QUARTER'] = dfcombined_cleaned_2015['QUARTER'].map(quarter_mapping).astype('category')\n",
    "\n",
    "# Set the categories and order for the QUARTER column\n",
    "dfcombined_cleaned_2015['QUARTER'] = dfcombined_cleaned_2015['QUARTER'].cat.set_categories(\n",
    "    ['Q1', 'Q2', 'Q3', 'Q4', 'OT1', 'OT2', 'OT3', 'OT4'], ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SEASON\n",
    "season_categories = dfcombined_cleaned['SEASON'].cat.categories\n",
    "season_mapping = {k: v for k, v in enumerate(season_categories)}\n",
    "\n",
    "\n",
    "# For QUARTER\n",
    "quarter_categories = dfcombined_cleaned['QUARTER'].cat.categories\n",
    "quarter_mapping = {k: v for k, v in enumerate(quarter_categories)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SEASON\n",
    "season_categories_2015 = dfcombined_cleaned_2015['SEASON'].cat.categories\n",
    "season_mapping_2015 = {k: v for k, v in enumerate(season_categories_2015)}\n",
    "\n",
    "\n",
    "# For QUARTER\n",
    "quarter_categories_2015 = dfcombined_cleaned_2015['QUARTER'].cat.categories\n",
    "quarter_mapping_2015 = {k: v for k, v in enumerate(quarter_categories_2015)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'SEASON' and 'QUARTER' to numeric codes\n",
    "dfcombined_cleaned['SEASON'] = dfcombined_cleaned['SEASON'].cat.codes\n",
    "dfcombined_cleaned['QUARTER'] = dfcombined_cleaned['QUARTER'].cat.codes\n",
    "\n",
    "# Add 1 to each to start the numbering from 1\n",
    "dfcombined_cleaned['SEASON'] += 1\n",
    "dfcombined_cleaned['QUARTER'] += 1\n",
    "\n",
    "# Verify the conversion by checking the data types again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'SEASON' and 'QUARTER' to numeric codes\n",
    "dfcombined_cleaned_2015['SEASON'] = dfcombined_cleaned_2015['SEASON'].cat.codes\n",
    "dfcombined_cleaned_2015['QUARTER'] = dfcombined_cleaned_2015['QUARTER'].cat.codes\n",
    "\n",
    "# Add 1 to each to start the numbering from 1\n",
    "dfcombined_cleaned_2015['SEASON'] += 1\n",
    "dfcombined_cleaned_2015['QUARTER'] += 1\n",
    "\n",
    "# Verify the conversion by checking the data types again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding of Categorical Variables\n",
    "\n",
    "##### Preparing Categorical Variables for Machine Learning\n",
    "To facilitate the use of categorical data in machine learning models, we apply one-hot encoding. This process converts categorical variables into a binary matrix representation, which is essential for models that require numerical input.\n",
    "\n",
    "##### Encoding Process\n",
    "- Initialize the `OneHotEncoder`.\n",
    "- Fit the encoder to the categorical columns and transform them into a sparse matrix to optimize memory.\n",
    "- Create a new DataFrame with encoded variables.\n",
    "\n",
    "##### Integrating Encoded Features\n",
    "- The original categorical columns are dropped from the cleaned DataFrame.\n",
    "- The new encoded features are concatenated with the remaining data.\n",
    "- A final check is performed to ensure no missing values are introduced during this process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON</th>\n",
       "      <th>SHOT_MADE</th>\n",
       "      <th>SHOT_TYPE</th>\n",
       "      <th>LOC_X</th>\n",
       "      <th>LOC_Y</th>\n",
       "      <th>SHOT_DISTANCE</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>IS_HOME_TEAM</th>\n",
       "      <th>DAY</th>\n",
       "      <th>TOTAL_TIME_LEFT</th>\n",
       "      <th>...</th>\n",
       "      <th>MONTH_10</th>\n",
       "      <th>MONTH_11</th>\n",
       "      <th>MONTH_12</th>\n",
       "      <th>WEEKDAY_1</th>\n",
       "      <th>WEEKDAY_2</th>\n",
       "      <th>WEEKDAY_3</th>\n",
       "      <th>WEEKDAY_4</th>\n",
       "      <th>WEEKDAY_5</th>\n",
       "      <th>WEEKDAY_6</th>\n",
       "      <th>WEEKDAY_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-22.2</td>\n",
       "      <td>12.15</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>16.85</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>37.25</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>7.05</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>20.75</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEASON  SHOT_MADE  SHOT_TYPE  LOC_X  LOC_Y  SHOT_DISTANCE  QUARTER  \\\n",
       "0       1          1          1  -22.2  12.15             23        5   \n",
       "1       1          1          0   -2.3  16.85             11        5   \n",
       "2       1          1          1    0.4  37.25             32        4   \n",
       "3       1          1          0   -1.3   7.05              2        5   \n",
       "4       1          1          1  -19.1  20.75             24        5   \n",
       "\n",
       "   IS_HOME_TEAM  DAY  TOTAL_TIME_LEFT  ...  MONTH_10  MONTH_11  MONTH_12  \\\n",
       "0             1   10               18  ...       0.0       0.0       0.0   \n",
       "1             0   10               27  ...       0.0       0.0       0.0   \n",
       "2             0   10                2  ...       0.0       0.0       0.0   \n",
       "3             1   10               46  ...       0.0       0.0       0.0   \n",
       "4             0   10               55  ...       0.0       0.0       0.0   \n",
       "\n",
       "   WEEKDAY_1  WEEKDAY_2  WEEKDAY_3  WEEKDAY_4  WEEKDAY_5  WEEKDAY_6  WEEKDAY_7  \n",
       "0        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "2        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "3        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "4        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 1090 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the categorical columns to be encoded\n",
    "categorical_columns = [\n",
    "    'TEAM_ID', 'PLAYER_ID', 'POSITION_GROUP', 'POSITION',\n",
    "    'ACTION_TYPE', 'BASIC_ZONE', 'ZONE_ABB', 'ZONE_RANGE', 'MONTH', 'WEEKDAY'\n",
    "]\n",
    "\n",
    "# Initialize the OneHotEncoder with sparse output\n",
    "encoder = OneHotEncoder(sparse=True, dtype=np.float32)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "encoded_data = encoder.fit_transform(dfcombined_cleaned[categorical_columns])\n",
    "\n",
    "# Get feature names for the encoded columns\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame.sparse.from_spmatrix(encoded_data, columns=encoded_columns)\n",
    "\n",
    "# Reset the index of the original DataFrame if necessary\n",
    "dfcombined_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Reset the index of the encoded DataFrame to match\n",
    "encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop the original categorical columns from the cleaned DataFrame\n",
    "dfcombined_cleaned = dfcombined_cleaned.drop(categorical_columns, axis=1)\n",
    "\n",
    "# Concatenate the encoded DataFrame with the original one, excluding the dropped categorical columns\n",
    "dfcombined_cleaned = pd.concat([dfcombined_cleaned, encoded_df], axis=1)\n",
    "\n",
    "# Check the first few rows of the resulting DataFrame\n",
    "display(dfcombined_cleaned.head())\n",
    "\n",
    "# Verify that there are no missing values after concatenation\n",
    "missing_values_after = dfcombined_cleaned.isnull().sum()\n",
    "\n",
    "coordinates_delete = ['LOC_X', 'LOC_Y']\n",
    "dfcombined_cleaned.drop(coordinates_delete, axis=1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON</th>\n",
       "      <th>SHOT_MADE</th>\n",
       "      <th>SHOT_TYPE</th>\n",
       "      <th>LOC_X</th>\n",
       "      <th>LOC_Y</th>\n",
       "      <th>SHOT_DISTANCE</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>IS_HOME_TEAM</th>\n",
       "      <th>DAY</th>\n",
       "      <th>TOTAL_TIME_LEFT</th>\n",
       "      <th>...</th>\n",
       "      <th>MONTH_10</th>\n",
       "      <th>MONTH_11</th>\n",
       "      <th>MONTH_12</th>\n",
       "      <th>WEEKDAY_1</th>\n",
       "      <th>WEEKDAY_2</th>\n",
       "      <th>WEEKDAY_3</th>\n",
       "      <th>WEEKDAY_4</th>\n",
       "      <th>WEEKDAY_5</th>\n",
       "      <th>WEEKDAY_6</th>\n",
       "      <th>WEEKDAY_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>5.35</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>14.85</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEASON  SHOT_MADE  SHOT_TYPE  LOC_X  LOC_Y  SHOT_DISTANCE  QUARTER  \\\n",
       "0       1          1          0   -0.4   6.75              1        4   \n",
       "1       1          1          0   -0.0   5.35              0        4   \n",
       "2       1          1          0   -0.0   5.35              0        5   \n",
       "3       1          1          0   -0.0   5.35              0        4   \n",
       "4       1          0          0   16.9  14.85             19        4   \n",
       "\n",
       "   IS_HOME_TEAM  DAY  TOTAL_TIME_LEFT  ...  MONTH_10  MONTH_11  MONTH_12  \\\n",
       "0             0   15               18  ...       0.0       0.0       0.0   \n",
       "1             0   15               22  ...       0.0       0.0       0.0   \n",
       "2             0   15                3  ...       0.0       0.0       0.0   \n",
       "3             0   15               37  ...       0.0       0.0       0.0   \n",
       "4             0   15               48  ...       0.0       0.0       0.0   \n",
       "\n",
       "   WEEKDAY_1  WEEKDAY_2  WEEKDAY_3  WEEKDAY_4  WEEKDAY_5  WEEKDAY_6  WEEKDAY_7  \n",
       "0        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "2        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "3        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "4        0.0        0.0        1.0        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[5 rows x 626 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the categorical columns to be encoded\n",
    "categorical_columns = [\n",
    "    'TEAM_ID', 'PLAYER_ID', 'POSITION_GROUP', 'POSITION',\n",
    "    'ACTION_TYPE', 'BASIC_ZONE', 'ZONE_ABB', 'ZONE_RANGE', 'MONTH', 'WEEKDAY'\n",
    "]\n",
    "\n",
    "# Initialize the OneHotEncoder with sparse output\n",
    "encoder_2015 = OneHotEncoder(sparse=True, dtype=np.float32)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "encoded_data_2015 = encoder_2015.fit_transform(dfcombined_cleaned_2015[categorical_columns])\n",
    "\n",
    "# Get feature names for the encoded columns\n",
    "encoded_columns_2015 = encoder_2015.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df_2015 = pd.DataFrame.sparse.from_spmatrix(encoded_data_2015, columns=encoded_columns_2015)\n",
    "\n",
    "# Reset the index of the original DataFrame if necessary\n",
    "dfcombined_cleaned_2015.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Reset the index of the encoded DataFrame to match\n",
    "encoded_df_2015.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop the original categorical columns from the cleaned DataFrame\n",
    "dfcombined_cleaned_2015 = dfcombined_cleaned_2015.drop(categorical_columns, axis=1)\n",
    "\n",
    "# Concatenate the encoded DataFrame with the original one, excluding the dropped categorical columns\n",
    "dfcombined_cleaned_2015 = pd.concat([dfcombined_cleaned_2015, encoded_df_2015], axis=1)\n",
    "\n",
    "# Check the first few rows of the resulting DataFrame\n",
    "display(dfcombined_cleaned_2015.head())\n",
    "\n",
    "# Verify that there are no missing values after concatenation\n",
    "missing_values_after_2015 = dfcombined_cleaned_2015.isnull().sum()\n",
    "\n",
    "coordinates_delete = ['LOC_X', 'LOC_Y']\n",
    "dfcombined_cleaned_2015.drop(coordinates_delete, axis=1, inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "After data cleaning, transformation, and optimization, the dataset is now fully preprocessed. We have addressed missing values, removed duplicates, transformed categorical variables into a machine-learning-friendly format using one-hot encoding, and optimized memory usage by adjusting data types. The dataset is now primed for the next phase of our analysis, which involves feature selection and model development. This structured and clean dataset forms a solid foundation for building robust predictive models and extracting meaningful insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your features and target\n",
    "X = dfcombined_cleaned.drop(['SHOT_MADE'], axis=1) #features\n",
    "y = dfcombined_cleaned['SHOT_MADE'] #target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your features and target\n",
    "X_test_2015 = dfcombined_cleaned_2015.drop(['SHOT_MADE'], axis=1) #features\n",
    "y_test_2015 = dfcombined_cleaned_2015['SHOT_MADE'] #target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets (e.g., 80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train and y_train are your training features and target\n",
    "# X_test and y_test are your testing features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Best Parameters from hyperparamter search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6381144199291721\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.85      0.71     87996\n",
      "           1       0.69      0.40      0.51     76346\n",
      "\n",
      "    accuracy                           0.64    164342\n",
      "   macro avg       0.66      0.62      0.61    164342\n",
      "weighted avg       0.65      0.64      0.62    164342\n",
      "\n",
      "Confusion Matrix:\n",
      " [[74465 13531]\n",
      " [45942 30404]]\n",
      "\n",
      "Test Accuracy: 0.6367663452223904\n",
      "\n",
      "Test AUC ROC score: 0.6214591554579363\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.84      0.71    109761\n",
      "           1       0.69      0.40      0.51     95666\n",
      "\n",
      "    accuracy                           0.64    205427\n",
      "   macro avg       0.65      0.62      0.61    205427\n",
      "weighted avg       0.65      0.64      0.62    205427\n",
      "\n",
      "Confusion Matrix:\n",
      " [[92699 17062]\n",
      " [57556 38110]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Best hyperparameters from the hyperparameter search\n",
    "best_params = {'learning_rate': 0.2, 'max_depth': 7 , 'colsample_bytree':0.9, 'subsample':0.8, 'min_child_weight':3}\n",
    "\n",
    "# Create an XGBoost classifier with the best hyperparameters\n",
    "best_xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    seed=42,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "# Train the model on the training set\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the validation set\n",
    "y_val_pred = best_xgb_model.predict(X_val)\n",
    "y_val_pred_binary = [1 if prob >= 0.5 else 0 for prob in y_val_pred]\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred_binary))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_val_pred_binary))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred_binary))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = best_xgb_model.predict(X_test)\n",
    "y_test_pred_binary = [1 if prob >= 0.5 else 0 for prob in y_test_pred]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_test_pred_binary))\n",
    "print(\"\\nTest AUC ROC score:\", roc_auc_score(y_test, y_test_pred_binary))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred_binary))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already trained your XGBoost model and obtained feature importances\n",
    "# Replace this with the actual feature importances from your model\n",
    "feature_importances = best_xgb_model.feature_importances_\n",
    "\n",
    "# Get the common features between training and test sets\n",
    "common_features = set(X_train.columns) & set(X_test_2015.columns)\n",
    "\n",
    "# Create a dictionary to store feature importances\n",
    "feature_importance_dict = {feature: 0 for feature in X_train.columns}\n",
    "\n",
    "# Assign the actual importances to common features\n",
    "for feature, importance in zip(X_train.columns, feature_importances):\n",
    "    if feature in common_features:\n",
    "        feature_importance_dict[feature] = importance\n",
    "\n",
    "# Now, feature_importance_dict contains importances for common features,\n",
    "# and 0 for features that are in the training set but not in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of Selected Features: 209\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already created feature_importance_dict as described in the previous answer\n",
    "\n",
    "# Get the features with non-zero importance\n",
    "selected_features = [feature for feature, importance in feature_importance_dict.items() if importance > 0]\n",
    "# print(\"Selected Features:\", selected_features)\n",
    "print(\"N of Selected Features:\",len(selected_features))\n",
    "# Filter the training and test sets to include only the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_val_selected = X_val[selected_features]\n",
    "X_test_selected = X_test_2015[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6356135376227623\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.83      0.71     87996\n",
      "           1       0.68      0.41      0.51     76346\n",
      "\n",
      "    accuracy                           0.64    164342\n",
      "   macro avg       0.65      0.62      0.61    164342\n",
      "weighted avg       0.65      0.64      0.62    164342\n",
      "\n",
      "Confusion Matrix:\n",
      " [[73354 14642]\n",
      " [45242 31104]]\n",
      "\n",
      "Test Accuracy: 0.6348789890768901\n",
      "\n",
      "Test AUC ROC score: 0.6101001032645949\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.85      0.72    113202\n",
      "           1       0.67      0.37      0.47     92234\n",
      "\n",
      "    accuracy                           0.63    205436\n",
      "   macro avg       0.65      0.61      0.60    205436\n",
      "weighted avg       0.64      0.63      0.61    205436\n",
      "\n",
      "Confusion Matrix:\n",
      " [[96547 16655]\n",
      " [58354 33880]]\n"
     ]
    }
   ],
   "source": [
    "# Train with early stopping\n",
    "best_xgb_model.fit(X_train_selected, y_train)\n",
    "# Predictions\n",
    "# Predictions on the validation set\n",
    "y_val_pred = best_xgb_model.predict(X_val_selected)\n",
    "y_val_pred_binary = [1 if prob >= 0.5 else 0 for prob in y_val_pred]\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred_binary))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_val_pred_binary))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred_binary))\n",
    "\n",
    "# Predictions on the test set\n",
    "y_test_pred = best_xgb_model.predict(X_test_selected)\n",
    "y_test_pred_binary = [1 if prob >= 0.5 else 0 for prob in y_test_pred]\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_2015, y_test_pred_binary))\n",
    "print(\"\\nTest AUC ROC score:\", roc_auc_score(y_test_2015, y_test_pred_binary))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_2015, y_test_pred_binary))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_2015, y_test_pred_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
